# -*- coding: utf-8 -*-
"""BERT_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p9sau8krlG-P7nF3IYZfBq0qGba2oQtC
"""

!pip install -q --upgrade keras-nlp tensorflow

import os
import keras_nlp
import tensorflow as tf
from tensorflow import keras
policy = keras.mixed_precision.Policy("mixed_float16")
keras.mixed_precision.set_global_policy(policy)

# Downloading Pre-Training data
keras.utils.get_file(
    origin="https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip",
    extract=True,
)


# Downloading Fine-Tuning Data
wiki_dir = os.path.expanduser("~/.keras/datasets/wikitext-103-raw/")


keras.utils.get_file(
    origin = "https://dl.fbaipublicfiles.com/glue/data/SST-2.zip",
    extract=True,
)

# Downloading Vocabulary Data
sst_dir = os.path.expanduser("~/.keras/datasets/SST-2/")

vocab_file = keras.utils.get_file(origin= "https://storage.googleapis.com/tensorflow/keras-nlp/examples/bert/bert_vocab_uncased.txt",
                                  )

# Prep_processing Parameters
PRETRAINING_BATCH_SIZE = 128
FINETUNING_BATCH_SIZE= 32
SEQ_LENGTH = 128
MASK_RATE = 0.25
PREDICTIONS_PER_SEQ = 32

# load sst-2
sst_train_ds = tf.data.experimental.CsvDataset(
    sst_dir + "train.tsv", [tf.string,tf.int32], header=True, field_delim= "\t"
).batch(FINETUNING_BATCH_SIZE)
sst_val_ds = tf.data.experimental.CsvDataset(
    sst_dir + "dev.tsv", [tf.string,tf.int32], header=True, field_delim= "\t"
).batch(FINETUNING_BATCH_SIZE)


# Loading Wiki-Text
wiki_train_ds = (
    tf.data.TextLineDataset(wiki_dir + "wiki.train.raw")
    .filter(lambda x: tf.strings.length(x)> 100)
    .batch(PRETRAINING_BATCH_SIZE)
)

wiki_val_ds = (
    tf.data.TextLineDataset(wiki_dir + "wiki.valid.raw")
    .filter(lambda x: tf.strings.length(x)> 100)
    .batch(PRETRAINING_BATCH_SIZE)
)

print(sst_train_ds.unbatch().batch(4).take(1).get_single_element())

# basline model with vectorization and simple bag of words

# This layer will turn our input sentence into a list of 1s and Os the same size
# our vocabulary, indicating whether a word is present in absent.

multi_hot_layer = keras.layers.TextVectorization(max_tokens= 4000, output_mode='multi_hot')
multi_hot_layer.adapt(sst_train_ds.map(lambda x, y:x))

# We then learn a linear regression over that layer, and that's our entire
# baseline model!
regression_layer = keras.layers.Dense(1, activation='sigmoid')

inputs = keras.Input(shape=(), dtype='string')
outputs = regression_layer(multi_hot_layer(inputs))
baseline_model = keras.Model(inputs, outputs)
baseline_model.compile(loss="binary_crossentropy", metrics=["accuracy"])
baseline_model.fit(sst_train_ds, validation_data= sst_val_ds, epochs=5)

# Tokenizer

# keras_nlp.tokenizers.WordPieceTokenizer(
#     vocabulary=None,
#     sequence_length: int=None,
#     lowercase: bool=False,
#     strip_accents: bool= False,
#     split: bool= True,
#     split_on_cjk: bool= True,
#     suffix_indicator: str = "##",
#     oov_token: str = "[UNK]",
#     **kwargs
# )

# vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
# inputs = ["The quick brown fox."]

# tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
#     vocabulary=vocab,
#     lowercase=False,
# )
# tokenizer(inputs)

# vocab = ["[UNK]", "the", "qu", "##ick", "br", "##own", "fox", "."]
# inputs = ["The quick brown fox."]

# tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
#     vocabulary=vocab,
#     lowercase=False,
#     dtype="string"
# )
# tokenizer(inputs)



"""Layer that applies language model masking
This layer is useful for preparing inputs for masked languaged modeling (MaskedLM) tasks. It follows the masking strategy
described in the original BERT paper. Given tokenized text, it randomly selects certain number of tokens for masking. Then for
each selected token, it has a chance (configurable) to be replaced by "mask token" or random token, or stay unchanged.
"""

# #Masked language generator


# keras_nlp.layers.MaskedLMMaskGenerator(
#     vocabulary_size,
#     mask_selection_rate,
#     mask_token_id,
#     mask_selection_length= None,
#     unselectable_token_ids=[0],
#     mask_token_rate= 0.8,
#     random_token_rate= 0.1,
#     **kwargs
# )

"""Pre-Training of Transformer
we will leverage the wikiText103 dataset, an unlabeled collection of wikipedia articles that is much
bigger than than SST-2.
We are going to train a transformer, a highly expressive model which will learn to embed each word in our input as a low
dimentional vector. Our wikipedia dataset has no labels, so we will use an unsupervised training objective called the Masked
Language Modeling (MaskedLM) ojective.
Essentially, we will be playing a big game of "guess the missing word". For each input sample we will obscure 25% of our input
data, and train our model to predict the parts we covered up.


Preprocess data for the MaskedLM task
Our text preprocessing for the MaskedLM task will occur in two stages.

Our text preprocessing for the MaskedLM task will occur in two stages.
1. Tokenize input text into integer sequences of token ids.
2. Mask certain positions in our input to predict on.
To tokenize, we can use a keras_nlp.tokenizers.Tokenizer - the KerasNLP building block for transforming text into sequences of integer token ids.


In particular, we will use keras_nIp. tokenizers .WordPieceTokenizer, which does sub-word tokenization. Sub-word
tokenization is popular when training models on large text corpora. Essentially, it allows our model to learn from uncommon
words, while not requiring a massive vocabulary of every word in our training set.

The second thing we need to do is mask our input for the MaskedLM task. To do this, we can use
keras_n1p. layers. MaskedLMMaskGenerator, which will randomly select a set of tokens in each input and mask them out.
The tokenizer and the masking laver can both be used inside a call to tf data Dataset map. We can use tf data to efficientlv
pre-compute each batch on the CPU, while our GPU or TPU works on training with the batch that came before. Because our  masking layer will choose new words to mask each time, each epoch over our dataset will give us a totally new set of labels to train on.
"""

# Preprocessing params.
PRETRAINING_BATCH_SIZE = 128
FINETUNING_BATCH_SIZE = 32
SEQ_LENGTH = 128
MASK_RATE = 0.25
PREDICTIONs_PER__SEQ = 32

# Model params.
NUM_LAYERS = 3
MODEL_DIM = 256
INTERMEDIATE_DIM = 512
NUM_HEADS = 4
DROPOUT = 0.1
NORM_EPSILON = 1e-5


# Training params.
PRETRAINING_LEARNING_RATE = 5e-4
PRETRAINING_EPOCHS = 8
FINETUNING_LEARNING_RATE = 5e-5
FINETUNING_EPOCHS = 3

"""Here block sorts our dataset into a (features, labels, weights) tuple, which can be passed directly to
keras.Model.fit() .
We have two features:
1. "token_ids", where some tokens have been replaced with our mask token id.
2. "mask _positions", which keeps track of which tokens we masked out.



Our labels are simply the ids we masked out.
Because not all sequences will have the same number of masks, we also keep a sample_weight tensor, which removes
padded labels from our loss function by giving them zero weight.

"""

# Setting sequence_length will trim or pad the token outputs to shape
# (batch_size, SEQ LENGTH) .
tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(
vocabulary=vocab_file,
sequence_length=SEQ_LENGTH,
lowercase=True,
strip_accents=True,
)

# Setting mask_selection_length will trim or pad the mask outputs to shape
# (batch_size, PREDICTIONS PER SEQ).
masker = keras_nlp.layers.MaskedLMMaskGenerator(
vocabulary_size=tokenizer. vocabulary_size(),
mask_selection_rate=MASK_RATE,
mask_selection_length=PREDICTIONS_PER_SEQ,
mask_token_id=tokenizer.token_to_id("[MASK]"),
)

def preprocess(inputs):
  inputs = tokenizer(inputs)
  outputs = masker(inputs)
# Split the masking layer outputs into a (features, labels, and weights)
# tuple that we can use with keras. Model. fit().
  features = {
      "token_ids": outputs["token_ids"],
      "mask_positions": outputs["mask_positions"],
  }
  labels = outputs ["mask_ids"]
  weights = outputs ["mask_weights"]
  return features, labels, weights

# We use prefetch() to pre-compute preprocessed batches on the fly on the CPU.
pretrain_ds = wiki_train_ds.map(
preprocess, num_parallel_calls=tf.data.AUTOTUNE
).prefetch(tf.data.AUTOTUNE)
pretrain_val_ds = wiki_val_ds.map(
preprocess, num_parallel_calls=tf.data.AUTOTUNE
).prefetch(tf.data.AUTOTUNE)

print(pretrain_val_ds.take(1).get_single_element())

"""â€¢ TransformerEncoder layer in KERASNLP"""

# #TransformerEncoder class
# keras_nlp.layers.TransformerEncoder(
#     intermediate_dim,
#     num_heads,
#     dropout=0,
#     activation="relu",
#     layer_norm_epsilon=1e-05,
#     kernel_initializer="glorot_uniform",
#     bias_initializer="zeros",
#     name=None,
#     **kwargs
# )

# # simple example of TokenAndPositionEmbedding
# keras_nlp.layers.tokenAndPositionEmbedding(
# vocabulary_size, #50k
# sequence length,#128
# embedding_dim, #256
# embeddings_initializer="glorot _uniform",
# mask_zero=False, #whether or not the input value 0 is a special "padding" value that should be masked out.
# **kwargs
# )

# # simple example Layernorm
# # You bring the data on the same scale and reduce variance.
# tf.keras.layers.LayerNormalization(
# axis=-1,
# epsilon=0.001,
# center=True,
# scale=True,
# beta_initializer="zeros",
# gamma_initializer="ones"
# beta_regularizer=None,
# gamma_regularizer=None,
# beta_constraint=None,
# gamma_constraint=None,
# **kwargs
# )

"""Dropout Layer
The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent
overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.
"""

# tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs)

"""Create the Transformer encoder.
We use keras_n1p. layers. TokenAndPositionEmbedding to first embed our input token ids. This layer simultaneously
learns two embeddings - one for words in a sentence and another for integer positions in a sentence. The output embedding is
simply the sum of the two.
Then we can add a series of keras_nIp. layers. TransformerEncoder layers. These are the bread and butter of the Transformer model, using an attention mechanism to attend to different parts of the input sentence, followed by a multi-layer
perceptron block.
The output of this model will be a encoded vector per input token id. Unlike the bag-of-words model we used as a baseline, this
model will embed each token accounting for the context in which it appeared.

"""

inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=tf. int32)

# Embed our tokens with a positional embedding.
embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(
vocabulary_size=tokenizer.vocabulary_size(),
sequence_length=SEQ_LENGTH,
embedding_dim=MODEL_DIM,
)
outputs = embedding_layer(inputs)

# Apply layer normalization and dropout to the embedding.
outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)
outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)

# Add al rimber of encoder blocks
for i in range(NUM_LAYERS):
  outputs = keras_nlp.layers.TransformerEncoder(
      intermediate_dim=INTERMEDIATE_DIM,
      num_heads=NUM_HEADS,
      dropout=DROPOUT,
      layer_norm_epsilon=NORM_EPSILON,
  )(outputs)


encoder_model = keras.Model(inputs, outputs)
encoder_model.summary()

"""Pretrain the Transformer
encoder_model is a modular unit, it is the piece of our model that we are really interested in for our
downstream task. However we still need to set up the encoder to train on the MaskedLM task; to do that we attach a
keras_nlp. layers.MaskedLMHead,
This layer will take as one input the token encodings, and as another the positions we masked out in the original input. It will
gather the token encodings we masked, and transform them back in predictions over our entire vocabulary.
"""

#pre-train the transformer
# Create the pretraining model by attaching a masked language model head.
inputs = {
    "token_ids": keras.Input(shape=(SEQ_LENGTH,), dtype=tf.int32),
    "mask_positions": keras.Input(shape=(PREDICTIONS_PER_SEQ,), dtype=tf.int32),
}

# Encode the tokens.
encoded_tokens = encoder_model(inputs["token_ids"])


# Predict an output word for each masked input token.
# We use the input token embedding to project from our encoded vectors to
# vocabulary logits, which has been shown to improve training efficiency.
outputs = keras_nlp.layers.MaskedLMHead(
embedding_weights=embedding_layer.token_embedding.embeddings,
activation="softmax",
)(encoded_tokens, mask_positions=inputs["mask_positions"])

# Define and compile pretraining model.
pretraining_model = keras.Model(inputs, outputs)
pretraining_model.compile(
loss="sparse_categorical_crossentropy",
optimizer= keras.optimizers.experimental.AdamW(PRETRAINING_LEARNING_RATE),
weighted_metrics=["sparse_categorical_accuracy"],
jit_compile=True,
)

# Pretrain the model  wiki text dataset.
pretraining_model.fit(
pretrain_ds,
validation_data = pretrain_val_ds,
epochs=PRETRAINING_EPOCHS,
)

encoder_model.save("encoder_model")

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/'My Drive'/my.csv /content/my.csv

!cp /content/drive/'My Drive'/encoder_model -r /content/encoder_model

!cp /content/encoder_model -r /content/drive/'My Drive'/encoder_model

restored_pre_trained_model_as_encoder_model = keras.models.load_model("encoder_model", compile=False)

"""Fine-tuning
After pretraining, we can now fine-tune our model on the SST-2 dataset. We can leverage the ability of the encoder we build to
predict on words in context to boost our our performance on the downstream task.

Preprocess data for downstream classification task of our pre-trained BERT model!
Preprocessing for fine-tuning is much simpler than for our retraining MaskedLM task. We just tokenize our input sentences
and we are ready for training!
"""

def preprocess(sentences, labels):
  return tokenizer(sentences), labels
# We use prefetch() to pre-compute preprocessed batches on the fly on our CPU.
finetune_ds = sst_train_ds.map(
preprocess, num_parallel_calls = tf.data.AUTOTUNE
).prefetch(tf.data.AUTOTUNE)

finetune_val_ds = sst_val_ds.map(
preprocess, num_parallel_calls=tf.data.AUTOTUNE
).prefetch(tf.data.AUTOTUNE)

# Preview a single input example.
print(finetune_val_ds.take(1).get_single_element())

"""Fine-tune the Transformer on downstream task: classification
To go from our encoded token output to a classification prediction, we need to attach another "head" to our Transformer model.
pool the encoded tokens together, and use a single dense layer to make a prediction.
"""

# Reload the encoder model from disk so we can restart fine-tuning from scratch.
encoder_model = keras.models.load_model("encoder_model", compile=False)
# Take as input the tokenized input.
inputs = keras. Input(shape=(SEQ_LENGTH,), dtype=tf.int32)
# Encode and pool the tokens.
encoded_tokens = encoder_model(inputs)
pooled_tokens = keras.layers.GlobalAveragePooling1D()(encoded_tokens)

# Predict an output label.
outputs = keras.layers.Dense(1, activation="sigmoid")(pooled_tokens)
# Define and compile our finetuning model.
finetuning_model = keras.Model(inputs, outputs)
finetuning_model.compile(
loss="binary_crossentropy",
optimizer=keras.optimizers.experimental.Adam (FINETUNING_LEARNING_RATE),
metrics=["accuracy"],
)

# Finetune the model for the SST-2 task.
finetuning_model.fit(
finetune_ds,
validation_data=finetune_val_ds,
epochs=FINETUNING_EPOCHS,
)

"""Save a model that accepts raw text
The last thing we can do with our fine-tuned model is saving including our tokenization layer. One of the key advantages of
KerasNLP is all preprocessing is done inside the tensorFlow graph, making it possible to save and restore a model that can
directly run inference on raw text!
"""

# Add our tokenization into our final model.
inputs = keras. Input(shape=(), dtype=tf.string)
tokens = tokenizer (inputs)
outputs = finetuning_model(tokens)
final_model = keras.Model(inputs, outputs)

final_model.save("final_model")

# This model can predict directly on raw text
# first reload our model from disk
restored_model = keras.models.load_model("final_model", compile=False)
#second start inference
inference_data = tf.constant (["Terrible, no good, trash.", "So great; I loved it!"])
print('Live Inference of our pre-trained and fine-tuned BERT model = ',restored_model.predict(inference_data))
print(predictions)

